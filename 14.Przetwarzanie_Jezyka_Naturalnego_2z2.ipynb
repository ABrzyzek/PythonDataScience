{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warsztaty Python w Data Science\n",
    "\n",
    "---\n",
    "## Przetwarzanie Języka Naturalnego - część 2 z 2  \n",
    "\n",
    "### Biblioteki NLP\n",
    "#### - Scikit-Learn\n",
    "#### - NLTK\n",
    "\n",
    "#### - Gensim\n",
    "#### - Spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn\n",
    "- Potężna biblioteka do nauczania maszynowego\n",
    "- Posiada wiele narzędzi do obróbki statystycznej danych tekstowych\n",
    "\n",
    "### NLTK\n",
    "- Biblioteka do przetwarzania języka naturalnego\n",
    "- Posiada wiele narzędzi do obróbki statystycznej danych tekstowych\n",
    "- KORPUSY\n",
    "- [NLTK Book](https://www.nltk.org/book/)\n",
    "\n",
    "### Spacy\n",
    "- Dużo wytrenowanych modeli statystycznych do języka\n",
    "- Chyba najlepsze modele do języka polskiego - morfologia, tagging (części mowy)\n",
    "\n",
    "### Gensim\n",
    "- Zbiór najnowszych algorytmów do obróbki danych tekstowych\n",
    "- Word2Vec, CBOW etc. etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('data\\gumtree-2021-03-09.csv', sep='|')\n",
    "columns = list(data.columns)\n",
    "columns[0] = \"Index\"\n",
    "data.columns=columns\n",
    "data.set_index('Index', drop=True, inplace=True)\n",
    "data.drop([\"title\", \"url\"], axis=1,inplace=True)\n",
    "\n",
    "def no_tags(s):\n",
    "    return re.sub(r'<[^<]+?>','',str(s))\n",
    "\n",
    "data[\"description\"] = data[\"description\"].apply(no_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Na sprzedaż piękna kawalerka o powierzchni 24 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mieszkanie dwupokojowe,własnościowe z 1971 r n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OPIS INWESTYCJI\\n===============\\nPOWER INVEST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bezpośrednio od dewelopera- brak prowizji 0%- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Na sprzedaż ekskluzywne mieszkanie dwupokojowe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description\n",
       "Index                                                   \n",
       "0      Na sprzedaż piękna kawalerka o powierzchni 24 ...\n",
       "1      Mieszkanie dwupokojowe,własnościowe z 1971 r n...\n",
       "2      OPIS INWESTYCJI\\n===============\\nPOWER INVEST...\n",
       "3      Bezpośrednio od dewelopera- brak prowizji 0%- ...\n",
       "4      Na sprzedaż ekskluzywne mieszkanie dwupokojowe..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sys\n",
    "import re\n",
    "\n",
    "f = gzip.open('data/odm.txt.gz', 'rt', encoding='utf-8')\n",
    "dictionary = {}\n",
    "\n",
    "for x in f:\n",
    "    t = x.strip().split(',')\n",
    "    tt = [ x.strip().lower() for x in t]\n",
    "    for w in tt[1:]: \n",
    "        dictionary[w]=tt[0]\n",
    "\n",
    "def lematize(w):\n",
    "    return dictionary.get(w,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "splitter = re.compile(r'[^ąąćęńłóóśśżżź\\w]+')\n",
    "isnumber = re.compile(r'[0-9]')\n",
    "\n",
    "def preprocessing(opis):\n",
    "    opis = str(opis)\n",
    "    \n",
    "    tokenized = splitter.split(opis)\n",
    "    l = list(tokenized)\n",
    "    l = [ x.lower() for x in l if len(x)>2 ]\n",
    "    l = [ x for x in l if isnumber.search(x) is None ]\n",
    "    l = [ lematize(x) for x in l ]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_description\"] = data[\"description\"].apply(lambda x: ' '.join(preprocessing(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>clean_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Na sprzedaż piękna kawalerka o powierzchni 24 ...</td>\n",
       "      <td>sprzedaż piękny kawalerka powierzchnia ostatni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mieszkanie dwupokojowe,własnościowe z 1971 r n...</td>\n",
       "      <td>mieszkać dwupokojowy własnościowy pierwszy pię...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OPIS INWESTYCJI\\n===============\\nPOWER INVEST...</td>\n",
       "      <td>opis inwestycja power invest przyjemność zapre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bezpośrednio od dewelopera- brak prowizji 0%- ...</td>\n",
       "      <td>bezpośredni deweloper brak prowizja brak podat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Na sprzedaż ekskluzywne mieszkanie dwupokojowe...</td>\n",
       "      <td>sprzedaż ekskluzywny mieszkać dwupokojowy powi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  \\\n",
       "Index                                                      \n",
       "0      Na sprzedaż piękna kawalerka o powierzchni 24 ...   \n",
       "1      Mieszkanie dwupokojowe,własnościowe z 1971 r n...   \n",
       "2      OPIS INWESTYCJI\\n===============\\nPOWER INVEST...   \n",
       "3      Bezpośrednio od dewelopera- brak prowizji 0%- ...   \n",
       "4      Na sprzedaż ekskluzywne mieszkanie dwupokojowe...   \n",
       "\n",
       "                                       clean_description  \n",
       "Index                                                     \n",
       "0      sprzedaż piękny kawalerka powierzchnia ostatni...  \n",
       "1      mieszkać dwupokojowy własnościowy pierwszy pię...  \n",
       "2      opis inwestycja power invest przyjemność zapre...  \n",
       "3      bezpośredni deweloper brak prowizja brak podat...  \n",
       "4      sprzedaż ekskluzywny mieszkać dwupokojowy powi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = {1: \"The game of life is a game of everlasting learning\", \n",
    "          2: \"The unexamined life is not worth living\", \n",
    "          3: \"Never stop learning\"}\n",
    "tfidf = TfidfVectorizer(min_df=2)\n",
    "tfs = tfidf.fit_transform(data[\"clean_description\"])\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aby', 'aczkolwiek', 'adaptacja', 'adres', 'agd', 'agencja', 'agent', 'aktualność', 'aktualny', 'aktywny', 'al', 'alejka', 'aluzyjny', 'amator', 'amfiteatr', 'andrychowicznieruchomosci', 'aneks', 'antresola', 'antywłamaniowy', 'apartament', 'apartamentowiec', 'apteka', 'aranżacja', 'aranżacyjny', 'architektoniczny', 'architektura', 'arkadia', 'armatura', 'art', 'atmosfera', 'atrakcja', 'atrakcjekomunikacja', 'atrakcyjny', 'atut', 'aut', 'autobus', 'autobusowy', 'bagno', 'bajkowy', 'balance', 'balkon', 'bank', 'bar', 'bardzo', 'basen', 'baza', 'bazarek', 'bazia', 'bem', 'bemowo']\n"
     ]
    }
   ],
   "source": [
    "print(list(feature_names[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tfs.toarray(), \n",
    "columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aby</th>\n",
       "      <th>aczkolwiek</th>\n",
       "      <th>adaptacja</th>\n",
       "      <th>adres</th>\n",
       "      <th>agd</th>\n",
       "      <th>agencja</th>\n",
       "      <th>agent</th>\n",
       "      <th>aktualność</th>\n",
       "      <th>aktualny</th>\n",
       "      <th>aktywny</th>\n",
       "      <th>...</th>\n",
       "      <th>świetny</th>\n",
       "      <th>świeży</th>\n",
       "      <th>żaden</th>\n",
       "      <th>żerać</th>\n",
       "      <th>żerań</th>\n",
       "      <th>żerańogłoszenie</th>\n",
       "      <th>życzyć</th>\n",
       "      <th>żyto</th>\n",
       "      <th>żyć</th>\n",
       "      <th>żłobek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.141596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061946</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093885</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.099201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.097644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.073988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.049656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224 rows × 1334 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aby  aczkolwiek  adaptacja     adres       agd  agencja  agent  \\\n",
       "0    0.000000         0.0        0.0  0.000000  0.000000      0.0    0.0   \n",
       "1    0.000000         0.0        0.0  0.000000  0.000000      0.0    0.0   \n",
       "2    0.000000         0.0        0.0  0.000000  0.000000      0.0    0.0   \n",
       "3    0.000000         0.0        0.0  0.000000  0.000000      0.0    0.0   \n",
       "4    0.117810         0.0        0.0  0.000000  0.061704      0.0    0.0   \n",
       "..        ...         ...        ...       ...       ...      ...    ...   \n",
       "219  0.099201         0.0        0.0  0.000000  0.000000      0.0    0.0   \n",
       "220  0.097644         0.0        0.0  0.000000  0.000000      0.0    0.0   \n",
       "221  0.073988         0.0        0.0  0.000000  0.000000      0.0    0.0   \n",
       "222  0.000000         0.0        0.0  0.000000  0.000000      0.0    0.0   \n",
       "223  0.049656         0.0        0.0  0.201406  0.000000      0.0    0.0   \n",
       "\n",
       "     aktualność  aktualny  aktywny  ...   świetny  świeży  żaden     żerać  \\\n",
       "0           0.0  0.000000      0.0  ...  0.000000     0.0    0.0  0.000000   \n",
       "1           0.0  0.000000      0.0  ...  0.000000     0.0    0.0  0.000000   \n",
       "2           0.0  0.000000      0.0  ...  0.000000     0.0    0.0  0.000000   \n",
       "3           0.0  0.000000      0.0  ...  0.000000     0.0    0.0  0.000000   \n",
       "4           0.0  0.059788      0.0  ...  0.044939     0.0    0.0  0.000000   \n",
       "..          ...       ...      ...  ...       ...     ...    ...       ...   \n",
       "219         0.0  0.100689      0.0  ...  0.000000     0.0    0.0  0.308448   \n",
       "220         0.0  0.099108      0.0  ...  0.000000     0.0    0.0  0.000000   \n",
       "221         0.0  0.075098      0.0  ...  0.000000     0.0    0.0  0.000000   \n",
       "222         0.0  0.000000      0.0  ...  0.000000     0.0    0.0  0.000000   \n",
       "223         0.0  0.000000      0.0  ...  0.000000     0.0    0.0  0.000000   \n",
       "\n",
       "     żerań  żerańogłoszenie    życzyć  żyto       żyć  żłobek  \n",
       "0      0.0          0.00000  0.000000   0.0  0.000000     0.0  \n",
       "1      0.0          0.00000  0.141596   0.0  0.000000     0.0  \n",
       "2      0.0          0.00000  0.000000   0.0  0.061946     0.0  \n",
       "3      0.0          0.00000  0.000000   0.0  0.093885     0.0  \n",
       "4      0.0          0.00000  0.000000   0.0  0.000000     0.0  \n",
       "..     ...              ...       ...   ...       ...     ...  \n",
       "219    0.0          0.12965  0.000000   0.0  0.000000     0.0  \n",
       "220    0.0          0.00000  0.000000   0.0  0.000000     0.0  \n",
       "221    0.0          0.00000  0.000000   0.0  0.000000     0.0  \n",
       "222    0.0          0.00000  0.000000   0.0  0.000000     0.0  \n",
       "223    0.0          0.00000  0.000000   0.0  0.000000     0.0  \n",
       "\n",
       "[224 rows x 1334 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLTK Downloader](img\\nltk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Na sprzedaż piękna kawalerka o powierzchni 24 m2 na ostatnim piętrze 10 piętrowego bloku z oknem wychodzącym na spokojną stronę osiedla. Bardzo dobrze skomunikowane z centrum (tramwaje ,autobusy).W pobliżu znajduje się dobra infrastruktura: sklepy, apteka, szkoła, targowisko ( hala Banacha),oraz park szczęśliwicki (5 minut na piechotę).Mieszkanie słoneczne i bardzo ustawne ,budynek po wymianie windy i elektryki w częściach wspólnych.Serdecznie zapraszamy do kontaktu.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opis = data['description'][0]\n",
    "opis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Na', 'sprzedaż', 'piękna', 'kawalerka', 'o', 'powierzchni', '24', 'm2', 'na', 'ostatnim', 'piętrze', '10', 'piętrowego', 'bloku', 'z', 'oknem', 'wychodzącym', 'na', 'spokojną', 'stronę', 'osiedla', '.', 'Bardzo', 'dobrze', 'skomunikowane', 'z', 'centrum', '(', 'tramwaje', ',', 'autobusy', ')', '.W', 'pobliżu', 'znajduje', 'się', 'dobra', 'infrastruktura', ':', 'sklepy', ',', 'apteka', ',', 'szkoła', ',', 'targowisko', '(', 'hala', 'Banacha', ')', ',', 'oraz', 'park', 'szczęśliwicki', '(', '5', 'minut', 'na', 'piechotę', ')', '.Mieszkanie', 'słoneczne', 'i', 'bardzo', 'ustawne', ',', 'budynek', 'po', 'wymianie', 'windy', 'i', 'elektryki', 'w', 'częściach', 'wspólnych.Serdecznie', 'zapraszamy', 'do', 'kontaktu', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(opis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Na sprzedaż piękna kawalerka o powierzchni 24 m2 na ostatnim piętrze 10 '\n",
      " 'piętrowego bloku z oknem wychodzącym na spokojną stronę osiedla.',\n",
      " 'Bardzo dobrze skomunikowane z centrum (tramwaje ,autobusy).W pobliżu '\n",
      " 'znajduje się dobra infrastruktura: sklepy, apteka, szkoła, targowisko ( hala '\n",
      " 'Banacha),oraz park szczęśliwicki (5 minut na piechotę).Mieszkanie słoneczne '\n",
      " 'i bardzo ustawne ,budynek po wymianie windy i elektryki w częściach '\n",
      " 'wspólnych.Serdecznie zapraszamy do kontaktu.']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sent_tokenize(opis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Na', 'sprzedaż', 'piękna', 'kawalerka', 'o', 'powierzchni', '24', 'm2', 'na', 'ostatnim', 'piętrze', '10', 'piętrowego', 'bloku', 'z', 'oknem', 'wychodzącym', 'na', 'spokojną', 'stronę', 'osiedla', '.']\n",
      "['Bardzo', 'dobrze', 'skomunikowane', 'z', 'centrum', '(', 'tramwaje', ',', 'autobusy', ')', '.W', 'pobliżu', 'znajduje', 'się', 'dobra', 'infrastruktura', ':', 'sklepy', ',', 'apteka', ',', 'szkoła', ',', 'targowisko', '(', 'hala', 'Banacha', ')', ',', 'oraz', 'park', 'szczęśliwicki', '(', '5', 'minut', 'na', 'piechotę', ')', '.Mieszkanie', 'słoneczne', 'i', 'bardzo', 'ustawne', ',', 'budynek', 'po', 'wymianie', 'windy', 'i', 'elektryki', 'w', 'częściach', 'wspólnych.Serdecznie', 'zapraszamy', 'do', 'kontaktu', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [ word_tokenize(sentence) for sentence in sent_tokenize(opis)]\n",
    "for sentence in tokens:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Katerina', 'Ivanovna'),\n",
       " ('Pyotr', 'Petrovitch'),\n",
       " ('Pulcheria', 'Alexandrovna'),\n",
       " ('Avdotya', 'Romanovna'),\n",
       " ('Rodion', 'Romanovitch'),\n",
       " ('Marfa', 'Petrovna'),\n",
       " ('Sofya', 'Semyonovna'),\n",
       " ('old', 'woman'),\n",
       " ('Project', 'Gutenberg-tm'),\n",
       " ('Porfiry', 'Petrovitch'),\n",
       " ('Amalia', 'Ivanovna'),\n",
       " ('great', 'deal'),\n",
       " ('young', 'man'),\n",
       " ('Nikodim', 'Fomitch'),\n",
       " ('Project', 'Gutenberg'),\n",
       " ('Ilya', 'Petrovitch'),\n",
       " ('Andrey', 'Semyonovitch'),\n",
       " ('Hay', 'Market'),\n",
       " ('Dmitri', 'Prokofitch'),\n",
       " ('Good', 'heavens')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n",
    "text.collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mock', 'Turtle'),\n",
       " ('said', 'Alice'),\n",
       " ('March', 'Hare'),\n",
       " ('White', 'Rabbit'),\n",
       " ('thought', 'Alice'),\n",
       " ('golden', 'key'),\n",
       " ('beautiful', 'Soup'),\n",
       " ('white', 'kid'),\n",
       " ('good', 'deal'),\n",
       " ('kid', 'gloves'),\n",
       " ('Mary', 'Ann'),\n",
       " ('yer', 'honour'),\n",
       " ('three', 'gardeners'),\n",
       " ('play', 'croquet'),\n",
       " ('Lobster', 'Quadrille'),\n",
       " ('ootiful', 'Soo'),\n",
       " ('great', 'hurry'),\n",
       " ('old', 'fellow'),\n",
       " ('trembling', 'voice'),\n",
       " ('poor', 'little'),\n",
       " ('next', 'witness'),\n",
       " ('feet', 'high'),\n",
       " ('poor', 'Alice'),\n",
       " ('inches', 'high'),\n",
       " ('young', 'lady')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice = nltk.corpus.gutenberg.fileids()[7]\n",
    "al = nltk.corpus.gutenberg.words(alice)\n",
    "al_text = nltk.Text(al)\n",
    "al_text.collocation_list(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pol'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "opis=\"Ala ma kota, kto tam przyszedł\"\n",
    "\n",
    "tc = nltk.classify.textcat.TextCat() \n",
    "tc.guess_language(opis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Spacy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install click==8.0.4\n",
    "!pip install spacy\n",
    "!python -m spacy download en\n",
    "!python -m spacy download pl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup VERB dep\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poczuł przyjemną woń mocnej kawy.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.pl.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poczuł VERB ROOT\n",
      "przyjemną ADJ amod\n",
      "woń NOUN obj\n",
      "mocnej ADJ amod\n",
      "kawy NOUN nmod:arg\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ala ma kota, kto tam przyszedł\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(opis)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ala PROPN nsubj\n",
      "ma VERB ROOT\n",
      "kota NOUN iobj\n",
      ", PUNCT punct\n",
      "kto PRON nsubj\n",
      "tam ADV advmod\n",
      "przyszedł VERB acl:relcl\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gensim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - model wektorowy słów w oparciu o sieci neuronowe (płytkie) - ale mówi się na to Deep Learning\n",
    "\n",
    "### vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7698541283607483)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755736470222473),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534157752991),\n",
       " ('prince', 0.6517035365104675),\n",
       " ('elizabeth', 0.6464517712593079),\n",
       " ('mother', 0.6311717629432678),\n",
       " ('emperor', 0.6106470823287964),\n",
       " ('wife', 0.6098655462265015)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czynniki wpływające na sukces ekstrakcji tematu\n",
    "\n",
    "1. Jakość wstępnej obróbki\n",
    "2. Róznorodnośc danych\n",
    "3. Długo, długo nic ...\n",
    "4. Dobór algorytmu\n",
    "5. Parametryzacja algorytmu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import nltk` \n",
    "\n",
    "`nltk.download('stopwords')`\n",
    "\n",
    "`nltk.download('wordnet')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\n",
    "         gzip.open('data/newsgroups.json.gz', 'rt', encoding='utf-8')\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rec.autos', 'comp.sys.mac.hardware', 'comp.graphics', 'sci.space',\n",
       "       'talk.politics.guns', 'sci.med', 'comp.sys.ibm.pc.hardware',\n",
       "       'comp.os.ms-windows.misc', 'rec.motorcycles', 'talk.religion.misc',\n",
       "       'misc.forsale', 'alt.atheism', 'sci.electronics', 'comp.windows.x',\n",
       "       'rec.sport.hockey', 'rec.sport.baseball', 'soc.religion.christian',\n",
       "       'talk.politics.mideast', 'talk.politics.misc', 'sci.crypt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target_names.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = df['content'][1]\n",
    "example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "\n",
    "for row in df.iterrows():\n",
    "    raw_data.append(row[1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "def lemmatize(word):\n",
    "    return wn_lemmatizer.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automaton'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize('automata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    result = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            result.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            result.append('SCREEN_NAME')\n",
    "        else:\n",
    "            result.append(token.lower_)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', ':', 'guykuo@carson.u.washington.edu', '(', 'guy', 'kuo', ')', 'subject', ':', 'si', 'clock', 'poll', '-', 'final', 'call', 'summary', ':', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', ':', 'si', ',', 'acceleration', ',', 'clock', ',', 'upgrade', 'article', '-', 'i.d.', ':', 'shelley.1qvfo9innc3s', 'organization', ':', 'university', 'of', 'washington', 'lines', ':', '11', 'nntp', '-', 'posting', '-', 'host', ':', 'URL', 'a', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', '.', 'please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', '.', 'top', 'speed', 'attained', ',', 'cpu', 'rated', 'speed', ',', 'add', 'on', 'cards', 'and', 'adapters', ',', 'heat', 'sinks', ',', 'hour', 'of', 'usage', 'per', 'day', ',', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '1.4', 'm', 'floppies', 'are', 'especially', 'requested', '.', 'i', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', ',', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'have', \"n't\", 'answered', 'this', 'poll', '.', 'thanks', '.', 'guy', 'kuo', '<', 'guykuo@u.washington.edu', '>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if len(token) < 14]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clock', 'final', 'summary', 'final', 'clock', 'report', 'keywords', 'acceleration', 'clock', 'upgrade', 'article', 'organization', 'university', 'washington', 'line', 'posting', 'number', 'brave', 'soul', 'upgraded', 'clock', 'oscillator', 'shared', 'experience', 'please', 'brief', 'message', 'detailing', 'experience', 'procedure', 'speed', 'attained', 'rated', 'speed', 'card', 'adapter', 'sink', 'usage', 'floppy', 'functionality', 'floppy', 'especially', 'requested', 'summarizing', 'please', 'network', 'knowledge', 'clock', 'upgrade', 'answered', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessing(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Zmieniłem z Markdown na Code - bo to trwa\n",
    "\n",
    "text_data = [ preprocessing(text) for text in raw_data ]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pikluj co się da!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Zmieniłem z Markdown na Code - bo to trwa\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(text_data, open('data/text_data.pkl', 'wb'))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "text_data = pickle.load(open('data/text_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clock', 'final', 'summary', 'final', 'clock', 'report', 'keywords', 'acceleration', 'clock', 'upgrade', 'article', 'organization', 'university', 'washington', 'line', 'posting', 'number', 'brave', 'soul', 'upgraded', 'clock', 'oscillator', 'shared', 'experience', 'please', 'brief', 'message', 'detailing', 'experience', 'procedure', 'speed', 'attained', 'rated', 'speed', 'card', 'adapter', 'sink', 'usage', 'floppy', 'functionality', 'floppy', 'especially', 'requested', 'summarizing', 'please', 'network', 'knowledge', 'clock', 'upgrade', 'answered', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "print(text_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "# corpus.MmCorpus.serialize('data\\corpus.mm', (x for x in corpus if len(x) > 0))\n",
    "# pickle.dump(corpus, open('data\\corpus.pkl', 'wb'))\n",
    "# dictionary.save('data\\dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "\n",
    "for w in corpus:\n",
    "    n+=1\n",
    "    if len(w)==0:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "corpus = pickle.load(open('data\\corpus.pkl', 'rb'))\n",
    "dictionary = pickle.load(open('data\\dictionary.gensim', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rec.autos', 'comp.sys.mac.hardware', 'comp.graphics', 'sci.space',\n",
       "       'talk.politics.guns', 'sci.med', 'comp.sys.ibm.pc.hardware',\n",
       "       'comp.os.ms-windows.misc', 'rec.motorcycles', 'talk.religion.misc',\n",
       "       'misc.forsale', 'alt.atheism', 'sci.electronics', 'comp.windows.x',\n",
       "       'rec.sport.hockey', 'rec.sport.baseball', 'soc.religion.christian',\n",
       "       'talk.politics.mideast', 'talk.politics.misc', 'sci.crypt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target_names.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.006*\"would\" + 0.006*\"people\" + 0.005*\"organization\" + 0.005*\"article\"')\n",
      "(1, '0.007*\"would\" + 0.007*\"people\" + 0.006*\"line\" + 0.006*\"christian\"')\n",
      "(2, '0.010*\"would\" + 0.009*\"writes\" + 0.009*\"organization\" + 0.009*\"line\"')\n",
      "(3, '0.014*\"line\" + 0.014*\"organization\" + 0.010*\"would\" + 0.010*\"writes\"')\n",
      "(4, '0.017*\"line\" + 0.016*\"organization\" + 0.010*\"window\" + 0.009*\"drive\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "\n",
    "ldamodel = gensim.models.ldamulticore.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=5)\n",
    "ldamodel.save('data/model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.006*\"would\" + 0.006*\"people\" + 0.005*\"organization\" + 0.005*\"article\" + 0.005*\"line\" + 0.005*\"space\" + 0.005*\"state\" + 0.004*\"writes\"')\n",
      "(1, '0.007*\"would\" + 0.007*\"people\" + 0.006*\"line\" + 0.006*\"christian\" + 0.005*\"organization\" + 0.005*\"jesus\" + 0.004*\"think\" + 0.004*\"writes\"')\n",
      "(2, '0.010*\"would\" + 0.009*\"writes\" + 0.009*\"organization\" + 0.009*\"line\" + 0.008*\"article\" + 0.007*\"people\" + 0.006*\"think\" + 0.005*\"university\"')\n",
      "(3, '0.014*\"line\" + 0.014*\"organization\" + 0.010*\"would\" + 0.010*\"writes\" + 0.009*\"article\" + 0.008*\"posting\" + 0.007*\"system\" + 0.005*\"could\"')\n",
      "(4, '0.017*\"line\" + 0.016*\"organization\" + 0.010*\"window\" + 0.009*\"drive\" + 0.008*\"university\" + 0.008*\"posting\" + 0.007*\"problem\" + 0.006*\"system\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=8)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target_names.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Zmień z Markdown na Code\n",
    "\n",
    "import gensim\n",
    "NUM_TOPICS = 20\n",
    "\n",
    "ldamodel = gensim.models.ldamulticore.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('data/model20.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel.load('data/model20.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.034*\"entry\" + 0.013*\"program\" + 0.011*\"rule\" + 0.010*\"section\" + 0.010*\"outlet\" + 0.007*\"neutral\" + 0.007*\"build\" + 0.007*\"output\"')\n",
      "(1, '0.026*\"window\" + 0.016*\"program\" + 0.013*\"image\" + 0.010*\"available\" + 0.010*\"version\" + 0.010*\"server\" + 0.010*\"file\" + 0.009*\"application\"')\n",
      "(2, '0.016*\"circuit\" + 0.015*\"power\" + 0.013*\"point\" + 0.012*\"ground\" + 0.011*\"radio\" + 0.011*\"input\" + 0.011*\"battery\" + 0.010*\"signal\"')\n",
      "(3, '0.020*\"armenian\" + 0.012*\"turkish\" + 0.012*\"people\" + 0.008*\"greek\" + 0.007*\"woman\" + 0.006*\"turkey\" + 0.006*\"turk\" + 0.006*\"armenia\"')\n",
      "(4, '0.035*\"president\" + 0.024*\"clinton\" + 0.015*\"koresh\" + 0.011*\"going\" + 0.009*\"today\" + 0.008*\"house\" + 0.008*\"package\" + 0.008*\"press\"')\n",
      "(5, '0.017*\"disease\" + 0.012*\"patient\" + 0.011*\"cause\" + 0.010*\"doctor\" + 0.009*\"medical\" + 0.008*\"safety\" + 0.008*\"effect\" + 0.007*\"thing\"')\n",
      "(6, '0.028*\"space\" + 0.015*\"SCREEN_NAME\" + 0.009*\"center\" + 0.008*\"earth\" + 0.008*\"orbit\" + 0.008*\"april\" + 0.008*\"research\" + 0.007*\"satellite\"')\n",
      "(7, '0.013*\"sweden\" + 0.011*\"finland\" + 0.010*\"jewish\" + 0.007*\"limbaugh\" + 0.007*\"helsinki\" + 0.007*\"stockholm\" + 0.005*\"finnish\" + 0.005*\"elizabeth\"')\n",
      "(8, '0.022*\"homosexual\" + 0.013*\"clayton\" + 0.013*\"virginia\" + 0.010*\"cramer\" + 0.009*\"number\" + 0.009*\"sexual\" + 0.009*\"homosexuality\" + 0.007*\"colormap\"')\n",
      "(9, '0.019*\"article\" + 0.019*\"writes\" + 0.013*\"line\" + 0.013*\"organization\" + 0.011*\"engine\" + 0.007*\"would\" + 0.007*\"speed\" + 0.007*\"motorcycle\"')\n",
      "(10, '0.064*\"drive\" + 0.017*\"controller\" + 0.014*\"speed\" + 0.013*\"system\" + 0.010*\"floppy\" + 0.009*\"apple\" + 0.008*\"jumper\" + 0.007*\"motherboard\"')\n",
      "(11, '0.019*\"would\" + 0.015*\"article\" + 0.014*\"writes\" + 0.014*\"organization\" + 0.013*\"line\" + 0.007*\"could\" + 0.007*\"first\" + 0.006*\"posting\"')\n",
      "(12, '0.023*\"western\" + 0.021*\"freenet\" + 0.017*\"reserve\" + 0.016*\"cleveland\" + 0.011*\"canon\" + 0.010*\"picture\" + 0.010*\"nubus\" + 0.008*\"cartridge\"')\n",
      "(13, '0.015*\"encryption\" + 0.014*\"clipper\" + 0.012*\"system\" + 0.009*\"public\" + 0.009*\"information\" + 0.008*\"security\" + 0.008*\"government\" + 0.008*\"algorithm\"')\n",
      "(14, '0.021*\"period\" + 0.017*\"georgia\" + 0.013*\"power\" + 0.008*\"michael\" + 0.008*\"athens\" + 0.006*\"scorer\" + 0.006*\"covington\" + 0.006*\"shot\"')\n",
      "(15, '0.037*\"line\" + 0.035*\"organization\" + 0.019*\"posting\" + 0.019*\"university\" + 0.013*\"would\" + 0.013*\"article\" + 0.012*\"writes\" + 0.010*\"anyone\"')\n",
      "(16, '0.013*\"would\" + 0.012*\"right\" + 0.010*\"people\" + 0.010*\"writes\" + 0.010*\"article\" + 0.009*\"state\" + 0.009*\"organization\" + 0.008*\"line\"')\n",
      "(17, '0.015*\"player\" + 0.013*\"line\" + 0.013*\"organization\" + 0.011*\"game\" + 0.010*\"hockey\" + 0.010*\"season\" + 0.009*\"writes\" + 0.009*\"university\"')\n",
      "(18, '0.014*\"would\" + 0.013*\"people\" + 0.008*\"think\" + 0.005*\"article\" + 0.005*\"thing\" + 0.005*\"writes\" + 0.005*\"money\" + 0.005*\"could\"')\n",
      "(19, '0.011*\"christian\" + 0.011*\"would\" + 0.010*\"people\" + 0.009*\"jesus\" + 0.009*\"line\" + 0.008*\"writes\" + 0.008*\"think\" + 0.008*\"organization\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=8)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rec.autos', 'comp.sys.mac.hardware', 'comp.graphics', 'sci.space',\n",
       "       'talk.politics.guns', 'sci.med', 'comp.sys.ibm.pc.hardware',\n",
       "       'comp.os.ms-windows.misc', 'rec.motorcycles', 'talk.religion.misc',\n",
       "       'misc.forsale', 'alt.atheism', 'sci.electronics', 'comp.windows.x',\n",
       "       'rec.sport.hockey', 'rec.sport.baseball', 'soc.religion.christian',\n",
       "       'talk.politics.mideast', 'talk.politics.misc', 'sci.crypt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target_names.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 0.84159905)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = 'Data science is a new technology that uses statistics and computer science'\n",
    "new_doc = preprocessing(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez = ldamodel.get_document_topics(new_doc_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 0.84159905)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez.sort(key=lambda x: x[1], reverse=True)\n",
    "rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 0.38689086), (10, 0.30832854), (1, 0.09367774), (17, 0.06815313), (9, 0.06580259), (19, 0.040835872), (2, 0.02355116)]\n"
     ]
    }
   ],
   "source": [
    "example_doc = preprocessing(example)\n",
    "example_bow = dictionary.doc2bow(example_doc)\n",
    "rez = ldamodel.get_document_topics(example_bow)\n",
    "rez.sort(key=lambda x: x[1], reverse=True)\n",
    "print(rez);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
